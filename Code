
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Load the dataset
linearX = pd.read_csv('C:/Users/KIIT/Documents/Assignment3/linearX.csv', header=None)
linearY = pd.read_csv('C:/Users/KIIT/Documents/Assignment3/linearY.csv', header=None)

# Step 2: Prepare the data
# Convert to numpy arrays for easier manipulation
X = linearX.values.flatten()
Y = linearY.values.flatten()

# Normalize X using z-score normalization for better convergence during training
X_mean = np.mean(X)
X_std = np.std(X)
X_normalized = (X - X_mean) / X_std

# Gradient Descent Function
def gradient_descent(X, Y, learning_rate, iterations):
    """
    Perform Batch Gradient Descent to find the optimal line for linear regression.
    """
    m = len(Y)  # Number of data points
    intercept, slope = 0, 0  # Initialize parameters
    cost_history = []  # Track cost values

    for _ in range(iterations):
        # Calculate predictions based on current parameters
        predictions = intercept + slope * X
        
        # Compute the cost (Mean Squared Error)
        cost = (1 / (2 * m)) * np.sum((predictions - Y) ** 2)
        cost_history.append(cost)

        # Calculate gradients
        gradient_intercept = (1 / m) * np.sum(predictions - Y)
        gradient_slope = (1 / m) * np.sum((predictions - Y) * X)

        # Update parameters
        intercept -= learning_rate * gradient_intercept
        slope -= learning_rate * gradient_slope

    return intercept, slope, cost_history

# Step 3: Run Batch Gradient Descent
learning_rate = 0.5
iterations = 1000
intercept, slope, cost_history = gradient_descent(X_normalized, Y, learning_rate, iterations)

# Display Results
print(f"Final Parameters:\nIntercept: {intercept}\nSlope: {slope}")
print(f"Final Cost Value: {cost_history[-1]:.6f}")

# Step 4: Plot Cost Function vs. Iterations
plt.figure(figsize=(10, 5))
plt.plot(range(1, 51), cost_history[:50], marker='o', color='b', label='Cost')
plt.title('Cost Function vs. Iterations (First 50 Iterations)')
plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.grid()
plt.legend()
plt.show()

# Step 5: Plot Dataset and Fitted Line
plt.figure(figsize=(10, 5))
plt.scatter(X_normalized, Y, color='r', label='Data Points')
regression_line = intercept + slope * X_normalized
plt.plot(X_normalized, regression_line, color='b', label='Regression Line')
plt.title('Dataset and Fitted Regression Line')
plt.xlabel('Normalized Predictor (X)')
plt.ylabel('Response (Y)')
plt.grid()
plt.legend()
plt.show()

# Step 6: Test Different Learning Rates
learning_rates = [0.005, 0.5, 5]
iterations = 50
plt.figure(figsize=(12, 8))
for lr in learning_rates:
    intercept_lr, slope_lr, cost_history_lr = gradient_descent(X_normalized, Y, lr, iterations)
    plt.plot(range(1, iterations + 1), cost_history_lr, label=f'lr = {lr}')

plt.title('Cost Function vs. Iterations for Different Learning Rates')
plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.legend()
plt.grid()
plt.show()

# Stochastic Gradient Descent Function
def stochastic_gradient_descent(X, Y, learning_rate, iterations):
    """
    Perform Stochastic Gradient Descent for linear regression.
    """
    m = len(Y)
    intercept, slope = 0, 0
    cost_history = []

    for _ in range(iterations):
        for i in range(m):
            # Prediction for the current point
            prediction = intercept + slope * X[i]

            # Calculate gradients for a single point
            gradient_intercept = prediction - Y[i]
            gradient_slope = (prediction - Y[i]) * X[i]

            # Update parameters
            intercept -= learning_rate * gradient_intercept
            slope -= learning_rate * gradient_slope

        # Compute cost after each epoch
        predictions = intercept + slope * X
        cost = (1 / (2 * m)) * np.sum((predictions - Y) ** 2)
        cost_history.append(cost)

    return intercept, slope, cost_history

# Run Stochastic Gradient Descent
intercept_sgd, slope_sgd, cost_history_sgd = stochastic_gradient_descent(X_normalized, Y, learning_rate=0.05, iterations=50)

# Plot Cost for Stochastic Gradient Descent
plt.figure(figsize=(10, 5))
plt.plot(range(1, 51), cost_history_sgd, marker='o', color='g', label='SGD Cost')
plt.title('Cost Function vs. Iterations (Stochastic Gradient Descent)')
plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.grid()
plt.legend()
plt.show()

# Mini-Batch Gradient Descent Function
def mini_batch_gradient_descent(X, Y, learning_rate, iterations, batch_size):
    """
    Perform Mini-Batch Gradient Descent for linear regression.
    """
    m = len(Y)
    intercept, slope = 0, 0
    cost_history = []

    for _ in range(iterations):
        for i in range(0, m, batch_size):
            X_batch = X[i:i + batch_size]
            Y_batch = Y[i:i + batch_size]

            # Predictions and gradients for the batch
            predictions = intercept + slope * X_batch
            gradient_intercept = (1 / len(Y_batch)) * np.sum(predictions - Y_batch)
            gradient_slope = (1 / len(Y_batch)) * np.sum((predictions - Y_batch) * X_batch)

            # Update parameters
            intercept -= learning_rate * gradient_intercept
            slope -= learning_rate * gradient_slope

        # Compute cost after each epoch
        predictions = intercept + slope * X
        cost = (1 / (2 * m)) * np.sum((predictions - Y) ** 2)
        cost_history.append(cost)

    return intercept, slope, cost_history

# Run Mini-Batch Gradient Descent
batch_size = 20
intercept_mbgd, slope_mbgd, cost_history_mbgd = mini_batch_gradient_descent(X_normalized, Y, learning_rate=0.05, iterations=50, batch_size=batch_size)

# Plot Cost for Mini-Batch Gradient Descent
plt.figure(figsize=(10, 5))
plt.plot(range(1, 51), cost_history_mbgd, marker='o', color='c', label='Mini-Batch GD Cost')
plt.title('Cost Function vs. Iterations (Mini-Batch Gradient Descent)')
plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.grid()
plt.legend()
plt.show()
